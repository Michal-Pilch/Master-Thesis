```{r}
library(caret)
library(ROCR)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(rpart)
library(MLeval)
library(modelplotr)
```


```{r}
train_result = data.frame(c(1:11))
control_tree = rpart.control(minbucket = 30, maxdepth = 20, cp = 0)
set.seed(1234)
```
Exerise 1:

```{r}
for (i in 0:10){
  if (i<=9){z = 10 *(2^i)} else { z = 9852}
  train_sample =churn_hw2_train[sample(nrow(churn_hw2_train), size = z),]
  train_data_logit = train(leave~., data = train_sample, method = "glm", family = binomial(link = "logit"))
  train_data_tree = train(leave~., data =train_sample, method = "rpart",parms = list(split ="information"), control = control_tree)
  train_result[(i+1),1] = max(train_data_tree$results$Accuracy)
  train_result[(i+1),2] = train_data_logit$results$Accuracy
  predict_tr = predict(train_data_tree, newdata = churn_hw2_train)
  predict_lg = predict(train_data_logit, newdata = churn_hw2_train )
  predict_tr = as.numeric(predict_tr)
  predict_lg = as.numeric(predict_lg)
  prediction_tr = prediction(predict_tr, churn_hw2_train$leave)
  prediction_lg = prediction(predict_lg, churn_hw2_train$leave)
  performance_tr = performance(prediction_tr,measure = "auc")
  performance_lg = performance(prediction_lg,measure = "auc")
  train_result[(i+1),3] =performance_tr@y.values[[1]]
  train_result[(i+1),4] =performance_lg@y.values[[1]]
  if (i<=9){train_result[(i+1),5] = 10 *(2^i)} else {train_result[(i+1),5] = 9852}
  
}
```

```{r}

train_result = train_result %>%  mutate(log_size = log(train_result$V5))
colnames(train_result) = c("Decision_Tree", "Logistic_Regression", "Tree_AUC", "Logit_AUC","Sample_Size","log_size")
ggplot(train_result, aes(x = log_size))+
  geom_line(aes(y = Decision_Tree, colour = "Decision Tree"))+
  geom_line(aes(y = Logistic_Regression,colour = "Logistic Regression"))+
  geom_point(aes(x =log_size[which.max(train_result$Decision_Tree)], y = max(train_result[,1:2]), colour = "Maximum (0.7674394)"))+
  labs(title = "Learning Curve", x = "Sample Size", y = "Accuracy", colour = "Model used")
ggplot(train_result, aes(x = log_size))+
  geom_line(aes(y = Tree_AUC, colour = "Tree AUC"))+
  geom_line(aes(y = Logit_AUC,colour = "Logit AUC"))+
  geom_point(aes(x = log_size[which.max(train_result$Tree_AUC)], y = max(train_result[,3:4]), colour = "Maximum (0.6616665)"))+
  labs(title = "Learning Curve", x = "Sample Size", y = "AUC", colour = "Model used")

```

Exercise 2:
```{r}
control = trainControl(method = "cv", number = 10, selectionFunction = "oneSE", savePredictions = TRUE, classProbs = T, summaryFunction = twoClassSummary)
tree_perf = data.frame((c(0:100)*10))
tree_cvv = trainControl(method = "cv", number = 5)
for (i in 0:100){
  z = i*10
  tree_control_cv = rpart.control(minsplit = z, maxdepth = 20, cp = 0)
  tree_cv_perf = train(leave~., data = churn_hw2_train, method = "rpart", trControl =tree_cvv , control = tree_control_cv)
  tree_perf[(i+1),1] =max(tree_cv_perf$results$Accuracy)
}
tree_perf = tree_perf %>%  mutate(size_of_leafs = ((c(0:100)*10)))
tree_control_best = rpart.control(minsplit = tree_perf[which.max(tree_perf$X.c.0.100....10.),2], maxdepth = 20, cp =0)
tree_cv= train(leave~., data = churn_hw2_train, method = "rpart",parms = list(split ="information"), control = tree_control_best, trControl = control)
eval_tree = evalm(tree_cv)
tree_cv= train(leave~., data = churn_hw2_train, method = "rpart",parms = list(split ="information"), control = control_tree, trControl = control)
var_imp = varImp(tree_cv, conditional=TRUE)
var_imp1 = data.frame(var_imp$importance)
var_imp1 = var_imp1 %>%  mutate(var_name = row.names(var_imp1))
var_imp2 = drop(var_imp1[var_imp1$Overall>0,])
var_imp2 = var_imp2[order(var_imp2$Overall, decreasing = TRUE),]
bar_plot = ggplot(data = var_imp2)+ geom_bar(stat = "identity",
    mapping = aes(x = reorder(var_name, Overall), y=Overall, fill = var_name), 
    show.legend = FALSE,
    width = 1)
bar_plot+coord_flip()

```






```{r}
tune_tree = rpart.control(minsplit = 30, maxdepth = 20, cp = 0)
model_best = train(leave~., data = churn_hw2_train, method = "rpart",control = tune_tree, parms = list(split = "information"))
model_knn = train(leave~., data = churn_hw2_train, method = "knn", tuneGrid = knn_grid)
sadn = prepare_scores_and_ntiles(
  datasets = list("churn_hw2_train", "churn_hw2_test"),
  dataset_labels = list("train","test"),
  models = list("model_best"),
  model_labels = list("Decision Tree"),
  target_column = "leave",
  ntiles = 20
)
plot_input = plotting_scope(prepared_input = sadn)
plot_costsrevs(data = plot_input, fixed_costs = 0, variable_costs_per_unit = 30,profit_per_unit = 68)

```

